{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d7ea524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import scipy\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "254228d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return jnp.where(x < 0, 5 + jnp.sum(jnp.sin(jnp.arange(1,5) * x[:, None]), axis=1), jnp.cos(10*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5b488a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_train=20, n_test=500):\n",
    "    xs_train = jnp.linspace(-jnp.pi, jnp.pi, n_train)\n",
    "    ys_train = f(xs_train)\n",
    "    xs_test = jnp.linspace(-jnp.pi, jnp.pi, n_test)\n",
    "    ys_test = f(xs_test)\n",
    "    return xs_train, ys_train, xs_test, ys_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90177a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "def init_network_params(layer_widths, key):\n",
    "    params = []\n",
    "    keys = random.split(key, len(layer_widths))\n",
    "    for i, (m, n) in enumerate(zip(layer_widths[:-1], layer_widths[1:])):\n",
    "        key, subkey = random.split(keys[i])\n",
    "        params.append((random.normal(key, (n, m)) * jnp.sqrt(2/m), random.normal(subkey, (n, 1)) * jnp.sqrt(2/m)))\n",
    "    return params\n",
    "\n",
    "def predict(params, x):\n",
    "    # Ensure x is a 2D array with each input as a row\n",
    "    x = x.reshape(-1, 1) if x.ndim == 1 else x\n",
    "\n",
    "    activations = x\n",
    "    for w, b in params[:-1]:\n",
    "        # Transpose w to align dimensions for matrix multiplication\n",
    "        activations = jnp.dot(activations, w.T)\n",
    "        # Add bias by broadcasting it properly. Reshape b to ensure it's a 1D array for correct broadcasting\n",
    "        activations = activations + b.reshape(-1)\n",
    "        activations = relu(activations)\n",
    "\n",
    "    final_w, final_b = params[-1]\n",
    "    y_pred = jnp.dot(activations, final_w.T) + final_b.reshape(-1)  # Ensure final_b is broadcasted correctly\n",
    "\n",
    "    return y_pred.flatten() if x.shape[1] == 1 else y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fb09db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(params, xs, ys):\n",
    "    preds = predict(params, xs)\n",
    "    return jnp.mean((preds - ys)**2)\n",
    "\n",
    "@jit\n",
    "def update(params, xs, ys, lr=0.01):\n",
    "    grads = grad(mse_loss)(params, xs, ys)\n",
    "    return [(w - lr * dw, b - lr*db) for (w, b), (dw, db) in zip(params, grads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f95e488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_relative_error(params, xs, ys):\n",
    "    preds = predict(params, xs)\n",
    "    return jnp.sqrt(jnp.sum((preds-ys) ** 2)) / jnp.sqrt(jnp.sum(ys ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50cada5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Width: 10, Mean L^2 Relative Error: 0.31912994384765625, Std Dev: 0.010569223202764988\n",
      "Width: 30, Mean L^2 Relative Error: 0.30348894000053406, Std Dev: 0.007711614016443491\n",
      "Width: 100, Mean L^2 Relative Error: 0.28334200382232666, Std Dev: 0.006841180846095085\n",
      "Width: 300, Mean L^2 Relative Error: 0.26906076073646545, Std Dev: 0.0013193837366998196\n",
      "Width: 1000, Mean L^2 Relative Error: 0.26286837458610535, Std Dev: 0.0017355125164613128\n"
     ]
    }
   ],
   "source": [
    "def train_network(params, xs, ys, n_epochs=1000, lr=0.01):\n",
    "    for epoch in range(n_epochs):\n",
    "        params = update(params, xs, ys, lr)\n",
    "    return params\n",
    "\n",
    "def test_network(params, xs_test, ys_test):\n",
    "    return l2_relative_error(params, xs_test, ys_test)\n",
    "\n",
    "def run_experiment(widths, n_train=20, n_test=500, n_epochs=20000, lr=0.0005, n_runs=3):\n",
    "    xs_train, ys_train, xs_test, ys_test = generate_data(n_train, n_test)\n",
    "    errors = {width: [] for width in widths}\n",
    "    \n",
    "    for width in widths:\n",
    "        for run in range(n_runs):\n",
    "            key = random.PRNGKey(run)\n",
    "            layer_widths = [1, width, 1] # input layer, hidden layer, output layer\n",
    "            params = init_network_params(layer_widths, key)\n",
    "            params = train_network(params, xs_train, ys_train, n_epochs, lr)\n",
    "            error = test_network(params, xs_test, ys_test)\n",
    "            errors[width].append(error)\n",
    "    \n",
    "    for width in widths:\n",
    "        mean_error = np.mean(errors[width])\n",
    "        std_error = np.std(errors[width])\n",
    "        print(f\"Width: {width}, Mean L^2 Relative Error: {mean_error}, Std Dev: {std_error}\")\n",
    "\n",
    "run_experiment(widths=[10, 30, 100, 300, 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe74beb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb11703d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
