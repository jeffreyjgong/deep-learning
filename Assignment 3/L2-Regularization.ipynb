{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13d6e59f",
   "metadata": {},
   "source": [
    "### Part b (L2 Regularization)\n",
    "\n",
    "#### Introduction\n",
    "Not too many changes from L1-Regularization, just implemented a new L2_Regularization function. \n",
    "\n",
    "#### Results\n",
    "I achieved a L2-relative error of 0.049 < 0.05, by using 100,000 epochs, a learning rate of 0.001, and a l2_lambda of 0.000001. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74482438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the oscillatory function\n",
    "def f(x):\n",
    "    result = torch.zeros_like(x)\n",
    "    result[x < 0] = 5 + sum(torch.sin(k * x[x < 0]) for k in range(1, 5))\n",
    "    result[x >= 0] = torch.cos(10 * x[x >= 0])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7ef4955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "x_train = torch.linspace(-np.pi, np.pi, 80)\n",
    "y_train = f(x_train) + torch.randn(x_train.size()) * 0.1  # Adding Gaussian noise\n",
    "\n",
    "# Generate testing data\n",
    "x_test = torch.linspace(-np.pi, np.pi, 1000)\n",
    "y_test = f(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "377fe0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "class ReLUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReLUNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 50)  # Input layer to hidden layer with 50 neurons\n",
    "        self.fc2 = nn.Linear(50, 50)  # Hidden layer to another hidden layer with 50 neurons\n",
    "        self.fc3 = nn.Linear(50, 1)  # Hidden layer to output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x.unsqueeze(1)))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12629b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model, loss function, and optimizer\n",
    "model = ReLUNet()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "l2_lambda = 0.000001  # Regularization strength\n",
    "# best so far: 0.00001\n",
    "\n",
    "# Function for L2 regularization\n",
    "def l2_regularization(model, loss):\n",
    "    l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "    loss += l2_lambda * l2_norm  # Apply L2 regularization\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b332ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.002871772041544318\n",
      "Epoch 500, Loss: 0.004121589940041304\n",
      "Epoch 1000, Loss: 0.002859259955585003\n",
      "Epoch 1500, Loss: 0.003803364234045148\n",
      "Epoch 2000, Loss: 0.002893748227506876\n",
      "Epoch 2500, Loss: 0.002955489093437791\n",
      "Epoch 3000, Loss: 0.002858104184269905\n",
      "Epoch 3500, Loss: 0.0030439188703894615\n",
      "Epoch 4000, Loss: 0.002980070188641548\n",
      "Epoch 4500, Loss: 0.0028636844363063574\n",
      "Epoch 5000, Loss: 0.0028091075364500284\n",
      "Epoch 5500, Loss: 0.0032219192944467068\n",
      "Epoch 6000, Loss: 0.002946931403130293\n",
      "Epoch 6500, Loss: 0.0029084894340485334\n",
      "Epoch 7000, Loss: 0.002954555908218026\n",
      "Epoch 7500, Loss: 0.002774035558104515\n",
      "Epoch 8000, Loss: 0.003045202698558569\n",
      "Epoch 8500, Loss: 0.0027621241752058268\n",
      "Epoch 9000, Loss: 0.0027557408902794123\n",
      "Epoch 9500, Loss: 0.0028272983618080616\n",
      "Epoch 10000, Loss: 0.003109192242845893\n",
      "Epoch 10500, Loss: 0.0027708448469638824\n",
      "Epoch 11000, Loss: 0.002856904175132513\n",
      "Epoch 11500, Loss: 0.0037994810845702887\n",
      "Epoch 12000, Loss: 0.0029933974146842957\n",
      "Epoch 12500, Loss: 0.0033315636683255434\n",
      "Epoch 13000, Loss: 0.0031760314013808966\n",
      "Epoch 13500, Loss: 0.0041240304708480835\n",
      "Epoch 14000, Loss: 0.0032275470439344645\n",
      "Epoch 14500, Loss: 0.0027818491216748953\n",
      "Epoch 15000, Loss: 0.0032077154610306025\n",
      "Epoch 15500, Loss: 0.002737029455602169\n",
      "Epoch 16000, Loss: 0.002734250156208873\n",
      "Epoch 16500, Loss: 0.0030985332559794188\n",
      "Epoch 17000, Loss: 0.00276247039437294\n",
      "Epoch 17500, Loss: 0.0032168044708669186\n",
      "Epoch 18000, Loss: 0.0026852518785744905\n",
      "Epoch 18500, Loss: 0.0032627235632389784\n",
      "Epoch 19000, Loss: 0.003138910513371229\n",
      "Epoch 19500, Loss: 0.0027873364742845297\n",
      "Epoch 20000, Loss: 0.0031215455383062363\n",
      "Epoch 20500, Loss: 0.003670951584354043\n",
      "Epoch 21000, Loss: 0.00278852297924459\n",
      "Epoch 21500, Loss: 0.0027166681829839945\n",
      "Epoch 22000, Loss: 0.0034112995490431786\n",
      "Epoch 22500, Loss: 0.0027092956006526947\n",
      "Epoch 23000, Loss: 0.0026829042471945286\n",
      "Epoch 23500, Loss: 0.002674448536708951\n",
      "Epoch 24000, Loss: 0.0026569662149995565\n",
      "Epoch 24500, Loss: 0.0031152861192822456\n",
      "Epoch 25000, Loss: 0.0029108275193721056\n",
      "Epoch 25500, Loss: 0.0028713394422084093\n",
      "Epoch 26000, Loss: 0.0027568666264414787\n",
      "Epoch 26500, Loss: 0.0030767631251364946\n",
      "Epoch 27000, Loss: 0.0026942382100969553\n",
      "Epoch 27500, Loss: 0.002673915820196271\n",
      "Epoch 28000, Loss: 0.0030784481205046177\n",
      "Epoch 28500, Loss: 0.0026890463195741177\n",
      "Epoch 29000, Loss: 0.0026421379297971725\n",
      "Epoch 29500, Loss: 0.002777820685878396\n",
      "Epoch 30000, Loss: 0.003484898479655385\n",
      "Epoch 30500, Loss: 0.002675822703167796\n",
      "Epoch 31000, Loss: 0.0026802215725183487\n",
      "Epoch 31500, Loss: 0.0026823733933269978\n",
      "Epoch 32000, Loss: 0.0026946072466671467\n",
      "Epoch 32500, Loss: 0.0026529571041464806\n",
      "Epoch 33000, Loss: 0.002661514561623335\n",
      "Epoch 33500, Loss: 0.0026735414285212755\n",
      "Epoch 34000, Loss: 0.0026794217992573977\n",
      "Epoch 34500, Loss: 0.0027727633714675903\n",
      "Epoch 35000, Loss: 0.002610135357826948\n",
      "Epoch 35500, Loss: 0.0027017032261937857\n",
      "Epoch 36000, Loss: 0.002741071628406644\n",
      "Epoch 36500, Loss: 0.0026534516364336014\n",
      "Epoch 37000, Loss: 0.00289526442065835\n",
      "Epoch 37500, Loss: 0.0028032835107296705\n",
      "Epoch 38000, Loss: 0.0035181983839720488\n",
      "Epoch 38500, Loss: 0.0029013839084655046\n",
      "Epoch 39000, Loss: 0.0030641446355730295\n",
      "Epoch 39500, Loss: 0.0030767396092414856\n",
      "Epoch 40000, Loss: 0.002660545753315091\n",
      "Epoch 40500, Loss: 0.0027870237827301025\n",
      "Epoch 41000, Loss: 0.002855779603123665\n",
      "Epoch 41500, Loss: 0.0029175630770623684\n",
      "Epoch 42000, Loss: 0.002960226498544216\n",
      "Epoch 42500, Loss: 0.002630002796649933\n",
      "Epoch 43000, Loss: 0.002600163221359253\n",
      "Epoch 43500, Loss: 0.0025894162245094776\n",
      "Epoch 44000, Loss: 0.003079834394156933\n",
      "Epoch 44500, Loss: 0.0029666624031960964\n",
      "Epoch 45000, Loss: 0.0026451940648257732\n",
      "Epoch 45500, Loss: 0.0027059654239565134\n",
      "Epoch 46000, Loss: 0.002613844582810998\n",
      "Epoch 46500, Loss: 0.002656866330653429\n",
      "Epoch 47000, Loss: 0.0029754219576716423\n",
      "Epoch 47500, Loss: 0.002565830945968628\n",
      "Epoch 48000, Loss: 0.004178428091108799\n",
      "Epoch 48500, Loss: 0.0025828818324953318\n",
      "Epoch 49000, Loss: 0.00275827432051301\n",
      "Epoch 49500, Loss: 0.002631842391565442\n",
      "Epoch 50000, Loss: 0.003025786252692342\n",
      "Epoch 50500, Loss: 0.0030616316944360733\n",
      "Epoch 51000, Loss: 0.002689586253836751\n",
      "Epoch 51500, Loss: 0.0026762261986732483\n",
      "Epoch 52000, Loss: 0.002640550024807453\n",
      "Epoch 52500, Loss: 0.0033193607814610004\n",
      "Epoch 53000, Loss: 0.0025700563564896584\n",
      "Epoch 53500, Loss: 0.0026109530590474606\n",
      "Epoch 54000, Loss: 0.002748811850324273\n",
      "Epoch 54500, Loss: 0.002633034950122237\n",
      "Epoch 55000, Loss: 0.0030420811381191015\n",
      "Epoch 55500, Loss: 0.0030910279601812363\n",
      "Epoch 56000, Loss: 0.0025422433391213417\n",
      "Epoch 56500, Loss: 0.0025461148470640182\n",
      "Epoch 57000, Loss: 0.0026317646261304617\n",
      "Epoch 57500, Loss: 0.00298920925706625\n",
      "Epoch 58000, Loss: 0.0026768208481371403\n",
      "Epoch 58500, Loss: 0.0028016739524900913\n",
      "Epoch 59000, Loss: 0.002544115763157606\n",
      "Epoch 59500, Loss: 0.002846025861799717\n",
      "Epoch 60000, Loss: 0.0026613844092935324\n",
      "Epoch 60500, Loss: 0.002542140893638134\n",
      "Epoch 61000, Loss: 0.0029002148658037186\n",
      "Epoch 61500, Loss: 0.0028316061943769455\n",
      "Epoch 62000, Loss: 0.002923704218119383\n",
      "Epoch 62500, Loss: 0.002846124581992626\n",
      "Epoch 63000, Loss: 0.002551295328885317\n",
      "Epoch 63500, Loss: 0.0025738244876265526\n",
      "Epoch 64000, Loss: 0.0027051465585827827\n",
      "Epoch 64500, Loss: 0.0028473271522670984\n",
      "Epoch 65000, Loss: 0.0026078398805111647\n",
      "Epoch 65500, Loss: 0.003199406433850527\n",
      "Epoch 66000, Loss: 0.002553765894845128\n",
      "Epoch 66500, Loss: 0.002712895395234227\n",
      "Epoch 67000, Loss: 0.0027028878685086966\n",
      "Epoch 67500, Loss: 0.00256358552724123\n",
      "Epoch 68000, Loss: 0.0028648353181779385\n",
      "Epoch 68500, Loss: 0.002568296855315566\n",
      "Epoch 69000, Loss: 0.0025604066904634237\n",
      "Epoch 69500, Loss: 0.0026961700059473515\n",
      "Epoch 70000, Loss: 0.002592653501778841\n",
      "Epoch 70500, Loss: 0.0025968526024371386\n",
      "Epoch 71000, Loss: 0.002652169205248356\n",
      "Epoch 71500, Loss: 0.00264634657651186\n",
      "Epoch 72000, Loss: 0.0029599629342556\n",
      "Epoch 72500, Loss: 0.0027431808412075043\n",
      "Epoch 73000, Loss: 0.0026236402336508036\n",
      "Epoch 73500, Loss: 0.0025004942435771227\n",
      "Epoch 74000, Loss: 0.002936645643785596\n",
      "Epoch 74500, Loss: 0.0027642997447401285\n",
      "Epoch 75000, Loss: 0.002556650899350643\n",
      "Epoch 75500, Loss: 0.0032069699373096228\n",
      "Epoch 76000, Loss: 0.0026574668008834124\n",
      "Epoch 76500, Loss: 0.0026655716355890036\n",
      "Epoch 77000, Loss: 0.002597491256892681\n",
      "Epoch 77500, Loss: 0.002438801107928157\n",
      "Epoch 78000, Loss: 0.002499169670045376\n",
      "Epoch 78500, Loss: 0.002451852895319462\n",
      "Epoch 79000, Loss: 0.0025263119023293257\n",
      "Epoch 79500, Loss: 0.0024234142620116472\n",
      "Epoch 80000, Loss: 0.002987163607031107\n",
      "Epoch 80500, Loss: 0.002715206937864423\n",
      "Epoch 81000, Loss: 0.0028561092913150787\n",
      "Epoch 81500, Loss: 0.0028605866245925426\n",
      "Epoch 82000, Loss: 0.002806158736348152\n",
      "Epoch 82500, Loss: 0.0023618866689503193\n",
      "Epoch 83000, Loss: 0.002447666833177209\n",
      "Epoch 83500, Loss: 0.002693007467314601\n",
      "Epoch 84000, Loss: 0.0025011373218148947\n",
      "Epoch 84500, Loss: 0.002384783234447241\n",
      "Epoch 85000, Loss: 0.0023313674610108137\n",
      "Epoch 85500, Loss: 0.002768279518932104\n",
      "Epoch 86000, Loss: 0.0023271706886589527\n",
      "Epoch 86500, Loss: 0.002397441305220127\n",
      "Epoch 87000, Loss: 0.0024173338897526264\n",
      "Epoch 87500, Loss: 0.002994426293298602\n",
      "Epoch 88000, Loss: 0.0026398098561912775\n",
      "Epoch 88500, Loss: 0.002413986250758171\n",
      "Epoch 89000, Loss: 0.0027527576312422752\n",
      "Epoch 89500, Loss: 0.002109888941049576\n",
      "Epoch 90000, Loss: 0.0028384304605424404\n",
      "Epoch 90500, Loss: 0.002080450300127268\n",
      "Epoch 91000, Loss: 0.0024621591437608004\n",
      "Epoch 91500, Loss: 0.0023644797038286924\n",
      "Epoch 92000, Loss: 0.002432038076221943\n",
      "Epoch 92500, Loss: 0.002315869787707925\n",
      "Epoch 93000, Loss: 0.0022452985867857933\n",
      "Epoch 93500, Loss: 0.002024815184995532\n",
      "Epoch 94000, Loss: 0.002476456807926297\n",
      "Epoch 94500, Loss: 0.0019882202614098787\n",
      "Epoch 95000, Loss: 0.001986186718568206\n",
      "Epoch 95500, Loss: 0.0019077814649790525\n",
      "Epoch 96000, Loss: 0.001892472617328167\n",
      "Epoch 96500, Loss: 0.002064520725980401\n",
      "Epoch 97000, Loss: 0.003012461820617318\n",
      "Epoch 97500, Loss: 0.0018719981890171766\n",
      "Epoch 98000, Loss: 0.002194741740822792\n",
      "Epoch 98500, Loss: 0.0019308949122205377\n",
      "Epoch 99000, Loss: 0.002008784795179963\n",
      "Epoch 99500, Loss: 0.002489398932084441\n",
      "Epoch 99999, Loss: 0.002562850248068571\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "epochs = 100000\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    loss = l2_regularization(model, loss)  # Apply L2 regularization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 500 == 0 or epoch == epochs-1:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80bc7b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Relative Error: 0.049379269480705264\n"
     ]
    }
   ],
   "source": [
    "# Calculate L2 relative error\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(x_test)\n",
    "    l2_norm = torch.sqrt(torch.sum((y_pred_test - y_test) ** 2))\n",
    "    f_norm = torch.sqrt(torch.sum(y_test ** 2))\n",
    "    l2_relative_error = l2_norm / f_norm\n",
    "    print(f'L2 Relative Error: {l2_relative_error.item()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
